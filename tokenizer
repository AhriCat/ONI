import re
from collections import defaultdict, Counter
import torch
from typing import Union, List, Dict, Optional
from tqdm import tqdm

class MultitokenBPETokenizer:
    def __init__(self, vocab_size=300000, max_merges=30000, n_future_tokens=4, return_tensors=None, padding=None, truncation=None, max_length=None):
        self.vocab_size = vocab_size
        self.max_merges = max_merges
        self.n_future_tokens = n_future_tokens
        self.special_tokens = {
            "[PAD]": 0, "[UNK]": 1, "[CLS]": 2, "[SEP]": 3, 
            "[MASK]": 4, "[EOS]": 5, "[USER]": 6, "[ASSISTANT]": 7, 
            "[SYSTEM]": 8, "[CONVERSATION]": 9, "[TEXT]": 10, "[VISION]": 11, 
            "[AUDIO]": 12, "[AGENTIC]": 13, "[EXPLORATORY]": 14, "[TASK-FOCUSED]": 15
        }
        self.token_to_id = self.special_tokens.copy()
        self.id_to_token = {v: k for k, v in self.token_to_id.items()}
        self.vocab = {}
        self.merges = {}
        self.pattern = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?[a-zA-Z]+| ?[0-9]+| ?[^\s\w]+|\s+(?!\S)|\s+""")
        self.pad_token = "[PAD]"
        self.pad_token_id = 0
        self.unk_token = "[UNK]"
        self.unk_token_id = 1
        self.return_tensors = return_tensors
        self.padding = padding
        self.truncation = truncation
        self.max_length = max_length or 4096

    def __call__(self, text: Union[str, List[str]], role: Optional[Union[str, List[str]]] = None, 
                 mode: Optional[Union[str, List[str]]] = None, modality: Optional[Union[str, List[str]]] = None,
                 return_tensors: Optional[str] = None, padding: Optional[Union[bool, str]] = None, 
                 truncation: Optional[bool] = None, max_length: Optional[int] = None) -> Dict[str, Union[List, torch.Tensor]]:
        if isinstance(text, str):
            text = [text]
        
        batch_size = len(text)
        roles = [role] * batch_size if isinstance(role, str) else role
        modes = [mode] * batch_size if isinstance(mode, str) else mode
        modalities = [modality] * batch_size if isinstance(modality, str) else modality

        encoded = [self.encode(t, r, m, mod) for t, r, m, mod in zip(text, roles or [None]*batch_size, modes or [None]*batch_size, modalities or [None]*batch_size)]
        
        max_len = max(len(e) for e in encoded) if padding == 'longest' else (max_length or self.max_length)
        
        padded_encoded = [self._pad_or_truncate(e, max_len) for e in encoded]
        attention_mask = [[1] * len(e) + [0] * (max_len - len(e)) for e in encoded]
        
        if return_tensors == 'pt':
            padded_encoded = torch.tensor(padded_encoded)
            attention_mask = torch.tensor(attention_mask)
        
        return {
            'input_ids': padded_encoded,
            'attention_mask': attention_mask
        }

    def build_vocab(self, token_counter: Counter, min_frequency: int = 1) -> Dict[str, int]:
        vocab = {char: freq for char, freq in token_counter.items() if freq >= min_frequency}
        for char in vocab:
            if char not in self.token_to_id:
                self.token_to_id[char] = len(self.token_to_id)
                self.id_to_token[len(self.id_to_token)] = char
        return vocab

    def train(self, texts: List[str], min_frequency: int = 1):
        token_counter = Counter()
        for text in tqdm(texts, desc="Tokenizing texts"):
            tokens = self.tokenize(text)
            token_counter.update(tokens)

        vocab = self.build_vocab(token_counter, min_frequency)

        pbar = tqdm(total=min(self.max_merges, self.vocab_size - len(self.token_to_id)), desc="Training BPE")
        while len(self.token_to_id) < self.vocab_size and len(self.merges) < self.max_merges:
            pairs = self.get_stats(vocab)
            if not pairs:
                break
            best = max(pairs, key=pairs.get)
            new_token = "".join(best)
            if new_token not in self.token_to_id:
                self.token_to_id[new_token] = len(self.token_to_id)
                self.id_to_token[len(self.id_to_token)] = new_token
            self.merges[best] = new_token
            vocab = self.merge_vocab(best, vocab)
            pbar.update(1)

        pbar.close()
        print(f"Final vocabulary size: {len(self.token_to_id)}")

    def get_stats(self, vocab: Dict[str, int]) -> Dict[tuple, int]:
        pairs = defaultdict(int)
        for word, freq in vocab.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[symbols[i], symbols[i + 1]] += freq
        return pairs

    def merge_vocab(self, pair: tuple, v_in: Dict[str, int]) -> Dict[str, int]:
        v_out = {}
        bigram = re.escape(' '.join(pair))
        p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
        for word in v_in:
            w_out = p.sub(''.join(pair), word)
            v_out[w_out] = v_in[word]
        return v_out

    def tokenize(self, text: str, role: Optional[str] = None, mode: Optional[str] = None, modality: Optional[str] = None) -> List[str]:
        role = role or "assistant"
        mode = mode or "conversation"
        modality = modality or "text"

        tokens = [
            f"[{role.upper()}]",
            f"[{mode.upper()}]",
            f"[{modality.upper()}]",
        ]
        tokens.extend([token for token in self.pattern.findall(text.lower()) if token.strip()])
        return tokens

    def encode(self, text: str, role: Optional[str] = None, mode: Optional[str] = None, 
               modality: Optional[str] = None, return_tensors: str = 'pt') -> Union[List[int], torch.Tensor]:
        tokens = self.tokenize(str(text), role, mode, modality)
        ids = []
        for token in tokens:
            if token in self.token_to_id:
                ids.append(self.token_to_id[token])
            else:
                for char in token:
                    ids.append(self.token_to_id.get(char, self.unk_token_id))
        ids.append(self.token_to_id['[EOS]'])
        
        if return_tensors == 'pt':
            return torch.tensor(ids, dtype=torch.long)
        return ids

    def encode_plus(self, text: str, role: Optional[str] = None, mode: Optional[str] = None, 
                    modality: Optional[str] = None, return_tensors: str = 'pt', 
                    max_length: Optional[int] = None, padding: bool = False) -> Dict[str, torch.Tensor]:
        ids = self.encode(text, role, mode, modality, return_tensors='pt')
        
        if max_length:
            ids = self._pad_or_truncate(ids, max_length)
        elif padding:
            ids = self._pad_or_truncate(ids, self.max_length)
        
        attention_mask = torch.ones_like(ids)
        
        return {
            'input_ids': ids.unsqueeze(0),
            'attention_mask': attention_mask.unsqueeze(0)
        }

    def _get_length(self, x: Union[List, torch.Tensor]) -> int:
        return len(x) if isinstance(x, list) else x.size(0)

    def _pad_or_truncate(self, ids: Union[List[int], torch.Tensor], max_length: int) -> torch.Tensor:
        if isinstance(ids, list):
            ids = torch.tensor(ids, dtype=torch.long)
        
        current_length = ids.size(0)
        
        if current_length > max_length:
            return ids[:max_length]
        
        padding_length = max_length - current_length
        padding = torch.full((padding_length,), self.pad_token_id, dtype=torch.long)
        return torch.cat([ids, padding])

    def batch_encode(self, texts: List[str], roles: Optional[List[str]] = None, 
                     modes: Optional[List[str]] = None, modalities: Optional[List[str]] = None, 
                     max_length: Optional[int] = None) -> Dict[str, torch.Tensor]:
        encoded_texts = [self.encode(text, role, mode, modality, return_tensors='pt') 
                         for text, role, mode, modality in zip(texts, roles or [None]*len(texts), 
                                                               modes or [None]*len(texts), 
                                                               modalities or [None]*len(texts))]
        
        max_len = max(self._get_length(e) for e in encoded_texts) if max_length is None else max_length
        padded_texts = [self._pad_or_truncate(e, max_len) for e in encoded_texts]
        
        attention_mask = torch.stack([torch.cat([torch.ones(self._get_length(e)), 
                                                 torch.zeros(max_len - self._get_length(e))]) 
                                      for e in encoded_texts])
        
        return {
            'input_ids': torch.stack(padded_texts),
            'attention_mask': attention_mask
        }
    def decode(self, ids: Union[List[int], torch.Tensor]) -> str:
        if isinstance(ids, torch.Tensor):
            ids = ids.tolist()
        tokens = [self.id_to_token[id] for id in ids if id in self.id_to_token and id != self.pad_token_id]
        tokens = [token for token in tokens if token not in self.special_tokens.keys()]
        return ''.join(tokens).replace('â–', ' ').strip()

    def batch_decode(self, batch_ids: Union[List[List[int]], torch.Tensor]) -> List[str]:
        if isinstance(batch_ids, torch.Tensor):
            batch_ids = batch_ids.tolist()
        return [self.decode(ids) for ids in batch_ids]

    def predict_future_tokens(self, text: str, model: torch.nn.Module, role: Optional[str] = None, 
                              mode: Optional[str] = None, modality: Optional[str] = None) -> List[str]:
        encoded_text = torch.tensor(self.encode(text, role, mode, modality)).unsqueeze(0)
        
        with torch.no_grad():
            outputs = model(encoded_text)
            logits = outputs.logits
        
        last_token_logits = logits[0, -1, :]
        top_k_values, top_k_indices = torch.topk(last_token_logits, self.n_future_tokens)
        
        return [self.id_to_token[idx.item()] for idx in top_k_indices]

    def generate_text(self, text: str, model: torch.nn.Module, role: Optional[str] = None, 
                      mode: Optional[str] = None, modality: Optional[str] = None, 
                      max_length: int = 100, temperature: float = 1.0) -> str:
        input_ids = torch.tensor(self.encode(text, role, mode, modality)).unsqueeze(0)
        
        for _ in range(max_length):
            with torch.no_grad():
                outputs = model(input_ids)
                next_token_logits = outputs.logits[0, -1, :] / temperature
                next_token_probs = torch.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(next_token_probs, num_samples=1)
                
                if next_token.item() == self.token_to_id['[EOS]']:
                    break
                
                input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)
        
        return self.decode(input_ids.squeeze())

    def save_pretrained(self, path: str):
        torch.save({
            'token_to_id': self.token_to_id,
            'id_to_token': self.id_to_token,
            'merges': self.merges,
            'vocab_size': self.vocab_size,
            'max_merges': self.max_merges,
            'n_future_tokens': self.n_future_tokens,
            'special_tokens': self.special_tokens,
        }, path)

    @classmethod
    def from_pretrained(cls, path: str):
        data = torch.load(path)
        tokenizer = cls(
            vocab_size=data['vocab_size'],
            max_merges=data['max_merges'],
            n_future_tokens=data['n_future_tokens']
        )
        tokenizer.token_to_id = data['token_to_id']
        tokenizer.id_to_token = data['id_to_token']
        tokenizer.merges = data['merges']
        tokenizer.special_tokens = data['special_tokens']
        return tokenizer

# Example usage
tokenizer = MultitokenBPETokenizer(vocab_size=10000, max_merges=1000, n_future_tokens=4)
