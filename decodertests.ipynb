{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install KerasNLP if not already installed\n",
    "# !pip install keras-nlp\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LEN = 128\n",
    "MIN_TRAINING_SEQ_LEN = 450\n",
    "\n",
    "# Model\n",
    "EMBED_DIM = 4096\n",
    "FEED_FORWARD_DIM = 16384\n",
    "NUM_HEADS = 32  # Aumentado para acomodar la nueva dimensiÃ³n de embedding\n",
    "NUM_LAYERS = 48 \n",
    "VOCAB_SIZE = 5000  # Limits parameters in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim debe ser divisible por num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = embed_dim // num_heads\n",
    "\n",
    "        self.wq = nn.Linear(embed_dim, embed_dim)\n",
    "        self.wk = nn.Linear(embed_dim, embed_dim)\n",
    "        self.wv = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.dense = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        query = self.split_heads(self.wq(x), batch_size)\n",
    "        key = self.split_heads(self.wk(x), batch_size)\n",
    "        value = self.split_heads(self.wv(x), batch_size)\n",
    "\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.depth)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention_weights, value)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.depth)\n",
    "        out = self.dense(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.linear1 = nn.Linear(embed_dim, feed_forward_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear2 = nn.Linear(feed_forward_dim, embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, memory, tgt_mask=None, memory_mask=None):\n",
    "        x2 = self.self_attn(x, x, x, attn_mask=tgt_mask)[0]\n",
    "        x = x + self.dropout1(x2)\n",
    "        x = self.norm1(x)\n",
    "        x2 = self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "        x = x + self.dropout2(x2)\n",
    "        x = self.norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the dataset\n",
    "keras.utils.get_file(\n",
    "    origin=\"https://storage.googleapis.com/asl-public/text/data/simplebooks.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "data_dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n",
    "\n",
    "# Load and filter the training dataset\n",
    "raw_train_ds = (\n",
    "    tf.data.TextLineDataset(data_dir + \"simplebooks-92-raw/train.txt\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .shuffle(buffer_size=256)\n",
    ")\n",
    "\n",
    "# Load and filter the validation dataset\n",
    "raw_val_ds = (\n",
    "    tf.data.TextLineDataset(data_dir + \"simplebooks-92-raw/valid.txt\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_dataset(dataset, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenizes a dataset of text examples.\n",
    "\n",
    "    Args:\n",
    "    - dataset (list of str): List of text examples to tokenize.\n",
    "    - max_length (int): Maximum length of the tokenized sequences.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing 'input_ids' and 'attention_mask' tensors.\n",
    "    \"\"\"\n",
    "    # Tokenize the dataset\n",
    "    encoding = tokenizer(\n",
    "        dataset,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    return encoding\n",
    "\n",
    "# Tokenize the training dataset\n",
    "def tokenize(dataset):\n",
    "    return dataset.map(lambda x: tokenizer(x))\n",
    "\n",
    "#train_ds = tokenize_dataset(raw_train_ds)\n",
    "#val_ds = tokenize_dataset(raw_val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, feed_forward_dim, num_layers, seq_len):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len, embed_dim))\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(embed_dim, num_heads, feed_forward_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, tgt_mask=None):\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, tgt_mask)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerModel(VOCAB_SIZE, EMBED_DIM, NUM_HEADS, FEED_FORWARD_DIM, NUM_LAYERS, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I am fine, thank you!\",\n",
    "    \"What are you doing today?\"\n",
    "]\n",
    "raw_val_ds = [\n",
    "    \"Hi, how's it going?\",\n",
    "    \"I'm good, what about you?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TensorDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m val_encodings \u001b[38;5;241m=\u001b[39m tokenizer(raw_val_ds, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create TensorDataset and DataLoader\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m(train_encodings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], train_encodings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m], train_encodings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      6\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(val_encodings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], val_encodings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m], val_encodings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      8\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TensorDataset' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(raw_train_ds, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(raw_val_ds, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], train_encodings[\"input_ids\"])\n",
    "val_dataset = TensorDataset(val_encodings[\"input_ids\"], val_encodings[\"attention_mask\"], val_encodings[\"input_ids\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        # Move data to the appropriate device (CPU/GPU)\n",
    "        input_ids = input_ids.to(model.device)\n",
    "        attention_mask = attention_mask.to(model.device)\n",
    "        labels = labels.to(model.device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
