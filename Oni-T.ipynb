{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7c3e8-9282-4619-9116-75f120a877b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.471435 M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import requests\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 64\n",
    "n_layer = 256\n",
    "dropout = 0.1\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "text = requests.get(url='https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y[0]\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        # Token and position embeddings\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, block_size, n_embd))\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=n_embd,\n",
    "                nhead=n_head,\n",
    "                dim_feedforward=4 * n_embd,\n",
    "                dropout=dropout,\n",
    "                activation='gelu'\n",
    "            ) for _ in range(n_layer)\n",
    "        ])\n",
    "\n",
    "        # Layer norm before the final linear layer\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # Decoder head\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "        # Token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(idx)\n",
    "        position_embeddings = self.position_embeddings[:, :T, :]\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        # Apply Transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # Calculate loss if targets are provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "    def generate(self, input_idx, max_length=50):\n",
    "        self.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            for _ in range(max_length):\n",
    "                logits, _ = self(input_idx)\n",
    "                # Get the logits of the last token in the sequence\n",
    "                probabilities = F.softmax(logits[:, -1, :], dim=-1)\n",
    "                # Sample from the distribution or pick the most likely\n",
    "                next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "                # Append to the sequence and use as the next input\n",
    "                input_idx = torch.cat([input_idx, next_token], dim=-1)\n",
    "\n",
    "                # Check if the last token is a stopping criterion, e.g., end of sequence token\n",
    "                if next_token.item() == stoi['<EOS>']:  # Assuming <EOS> is your end-of-sequence token\n",
    "                    break\n",
    "\n",
    "        return input_idx\n",
    "# Model parameters\n",
    "vocab_size = len(chars)  # Assuming `chars` is defined as before\n",
    "n_embd = 256\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "block_size = 128\n",
    "dropout = 0.1\n",
    "batch_size = 16348\n",
    "\n",
    "# Initialize model\n",
    "model = TransformerModel(vocab_size, n_embd, n_head, n_layer, block_size, dropout)\n",
    "model.train()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "#PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883611b-63a1-491b-b9b1-5c96b53c3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "# Training loop\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    knowledge_base_dir = ('C:/Users/jonny/Documents/PATH/ONI/knowledge_base/remembered_texts/')\n",
    "    train_data = os.path.join(knowledge_base_dir, 'train.txt')\n",
    "    val_data = os.path.join(knowledge_base_dir, 'val.txt')\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y[0]\n",
    "\n",
    "for epoch in range(10):  # Assuming a suitable number of epochs\n",
    "    for xb, yb in get_batch('train'):  # Assuming a data loader is defined\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(xb, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04467905-0d4b-49de-b243-36e527447e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Placeholder functions for your custom model's text processing\n",
    "def process_input_text(input_text):\n",
    "    # Implement this to convert input_text to the format your model expects\n",
    "    # For example, if the model expects tokenized input, tokenize the text here\n",
    "    return input_text\n",
    "\n",
    "def generate_response(model, text_input, image_input=None, audio_input=None, video_input=None):\n",
    "    # Ensure that we provide four inputs to the model\n",
    "    if image_input is None:\n",
    "        image_input = np.zeros((1, 1920, 1080, 3), dtype=np.float32)  # Dummy image input\n",
    "    if audio_input is None:\n",
    "        audio_input = np.zeros((1, 1000, 1), dtype=np.float32)        # Dummy audio input\n",
    "    if video_input is None:\n",
    "        video_input = np.zeros((1, 225, 225, 3), dtype=np.float32)    # Dummy video input\n",
    "\n",
    "    text_input = np.array([text_input])  # Ensure text input is in array form\n",
    "    inputs = [text_input, image_input, audio_input, video_input]\n",
    "    response = model.predict(inputs)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Function to handle video capture\n",
    "def capture_video_frame():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if ret:\n",
    "        frame = cv2.resize(frame, (225, 225))  # Resize the frame to match the expected size\n",
    "        return frame\n",
    "    else:\n",
    "        return np.zeros((225, 225, 3), dtype=np.float32)  # Dummy video input\n",
    "\n",
    "\n",
    "os.environ['DISPLAY'] = ':1'\n",
    "\n",
    "# Initialize the Tkinter root\n",
    "root = tk.Tk()\n",
    "root.title(\"OniChat\")\n",
    "\n",
    "# Create the conversation display\n",
    "conversation_frame = tk.Frame(root)\n",
    "conversation_frame.pack(pady=10)\n",
    "\n",
    "conversation_list = tk.Listbox(conversation_frame, width=80, height=20)\n",
    "conversation_list.pack(side=tk.LEFT, fill=tk.BOTH)\n",
    "\n",
    "scrollbar = tk.Scrollbar(conversation_frame, orient=tk.VERTICAL)\n",
    "scrollbar.config(command=conversation_list.yview)\n",
    "scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "\n",
    "conversation_list.config(yscrollcommand=scrollbar.set)\n",
    "\n",
    "# Function to handle user input and generate response\n",
    "def send_message(event=None):\n",
    "    user_input = user_entry.get()\n",
    "    if user_input:\n",
    "        conversation_list.insert(tk.END, f\"You: {user_input}\")\n",
    "        user_entry.delete(0, tk.END)\n",
    "        \n",
    "        # Process the input and generate response using the custom model\n",
    "        processed_input = process_input_text(user_input)\n",
    "        \n",
    "        # Capture video frame\n",
    "        video_frame = capture_video_frame()\n",
    "        \n",
    "        response_text = generate_response(trans_model, processed_input, video_input=video_frame)\n",
    "        \n",
    "        conversation_list.insert(tk.END, f\"Bot: {response_text}\")\n",
    "        conversation_list.yview(tk.END)  # Scroll to the bottom\n",
    "\n",
    "# Create the entry widget for user input\n",
    "user_entry = tk.Entry(root, width=80)\n",
    "user_entry.pack(pady=10)\n",
    "user_entry.bind(\"<Return>\", send_message)\n",
    "\n",
    "# Create the send button\n",
    "send_button = tk.Button(root, text=\"Send\", command=send_message)\n",
    "send_button.pack()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                print(page.extract_text())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file {file_path}: {e}\")\n",
    "\n",
    "def read_docx(file_path):\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        for para in doc.paragraphs:\n",
    "            print(para.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading DOCX file {file_path}: {e}\")\n",
    "\n",
    "# Function to handle file upload\n",
    "def upload_file():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        conversation_list.insert(tk.END, f\"File uploaded: {file_path}\")\n",
    "        if file_path.endswith('.pdf'):\n",
    "            read_pdf(file_path)\n",
    "        elif file_path.endswith('.docx'):\n",
    "            read_docx(file_path)\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = file.read()\n",
    "                conversation_list.insert(tk.END, f\"File content: {data[:100]}...\")  # Display first 100 characters\n",
    "                processed_input = process_input_text(data)\n",
    "                response_text = generate_response(trans_model, processed_input)\n",
    "                conversation_list.insert(tk.END, f\"Bot: {response_text}\")\n",
    "\n",
    "# Create the file upload button\n",
    "upload_button = tk.Button(root, text=\"Upload File\", command=upload_file)\n",
    "upload_button.pack()\n",
    "\n",
    "# Initialize the Ears class with a proper callback\n",
    "ears = Ears(audio_callback=Ears.audio_callback)\n",
    "\n",
    "# Example usage\n",
    "read_pdf('C:/Users/jonny/Documents/PATH/ONI/knowledge_base/remembered_texts/')\n",
    "read_docx('C:/Users/jonny/Documents/PATH/ONI/knowledge_base/remembered_texts/')\n",
    "# Function to handle microphone input\n",
    "def record_audio(ears):\n",
    "    conversation_list.insert(tk.END, \"Listening...\")\n",
    "    ears = Ears\n",
    "    ears.start_listening()\n",
    "    time.sleep(10)  # Listen for 10 seconds (adjust as needed)\n",
    "    ears.stream.stop()\n",
    "    conversation_list.insert(tk.END, \"Stopped listening\")\n",
    "\n",
    "\n",
    "# Create the microphone button\n",
    "microphone_button = tk.Button(root, text=\"Microphone\", command=record_audio)\n",
    "microphone_button.pack()\n",
    "\n",
    "# Training loop to use files from ./knowledge_base\n",
    "def training_loop(model, knowledge_base_dir):\n",
    "    for filename in os.listdir(knowledge_base_dir):\n",
    "        file_path = os.path.join(knowledge_base_dir, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "               with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    data = file.read()\n",
    "                    processed_input = process_input_text(data)\n",
    "                    # We will use dummy values for other inputs during training\n",
    "            except UnicodeDecodeError:\n",
    "                    print(f\"Error decoding file: {file_path}\")\n",
    "                    processed_input = process_input_text(file_path)\n",
    "                    # We will use dummy values for other inputs during training\n",
    "                    image_input = np.zeros((1, 1920, 1080, 3), dtype=np.float32)\n",
    "                    audio_input = np.zeros((1, 1000, 1), dtype=np.float32)\n",
    "                    video_input = np.zeros((1, 225, 225, 3), dtype=np.float32)\n",
    "                                \n",
    "                # Train the model\n",
    "                #model.train_on_batch([processed_input, image_input, audio_input, video_input], processed_input)  # Adjust this line based on your model's training method\n",
    "\n",
    "# Path to the knowledge base directory\n",
    "\n",
    "knowledge_base_dir = ('C:/Users/jonny/Documents/PATH/ONI/knowledge_base/remembered_texts/')\n",
    "# Start the training loop\n",
    "training_loop(model, knowledge_base_dir)\n",
    "\n",
    "# Run the Tkinter main loop\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
